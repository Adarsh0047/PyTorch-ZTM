{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxYshXXnb503"
      },
      "source": [
        "## 1. Quick Code Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT5BqXWCb505"
      },
      "source": [
        "### 1.1 Before Pytorch 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5OH-Oldbb505"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet50() # note: this could be any model\n",
        "\n",
        "### Train model ###\n",
        "\n",
        "### Test model ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crzhpWUub506"
      },
      "source": [
        "### 1.2 After Pytorch 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OZo4kdzPb506"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet50() # note: this could be any model\n",
        "compiled_model = torch.compile(model) # <- magic happens!\n",
        "\n",
        "### Train model ### <- faster!\n",
        "\n",
        "### Test model ### <- faster!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpaQoRtSb506"
      },
      "source": [
        "## 2. Getting info from gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHeThCoGb507",
        "outputId": "819f1a15-fbb7-4cc7-c631-b4b54fc5bc2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name: Tesla_T4\n",
            "GPU capability score: (7, 5)\n",
            "GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\n",
            "GPU information:\n",
            "Wed Sep 20 02:34:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKmh6uc_b507"
      },
      "source": [
        "### 2.1 Globally Set devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTz65so8b507",
        "outputId": "6e7e30fd-7d2f-436d-a830-fbd09342106c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer weights are on device: cuda:0\n",
            "Layer creating data on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set the device with context manager (requires PyTorch 2.x+)\n",
        "with torch.device(device):\n",
        "    # All tensors created in this block will be on device\n",
        "    layer = torch.nn.Linear(20, 30)\n",
        "    print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5jXuHWfb507",
        "outputId": "32bc6749-25a1-427a-d201-a67dd5877095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer weights are on device: cuda:0\n",
            "Layer creating data on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set the device globally\n",
        "torch.set_default_device(device)\n",
        "\n",
        "# All tensors created will be on the global device by default\n",
        "layer = torch.nn.Linear(20, 30)\n",
        "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setting up experiments"
      ],
      "metadata": {
        "id": "GF8PFCSBb507"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TorchVision version: {torchvision.__version__}\")\n",
        "\n",
        "# Set the target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTZGyDf7dngi",
        "outputId": "274e874e-e052-4ed9-9bb7-438fdf43d598"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "TorchVision version: 0.15.2+cu118\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create models and transforms"
      ],
      "metadata": {
        "id": "yMGy7zp2dpDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model weights and transforms\n",
        "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # <- use the latest weights (could also use .DEFAULT)\n",
        "transforms = model_weights.transforms()\n",
        "\n",
        "# Setup model\n",
        "model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "# Count the number of parameters in the model\n",
        "total_params = sum(\n",
        "    param.numel() for param in model.parameters() # <- all params\n",
        "\t# param.numel() for param in model.parameters() if param.requires_grad # <- only trainable params\n",
        ")\n",
        "\n",
        "print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\n",
        "print(f\"Model transforms:\\n{transforms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pP0JhmTdtBl",
        "outputId": "18567f3b-2e80-4e1c-b1dd-801285f6d5c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 76.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters of model: 25557032 (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\n",
            "Model transforms:\n",
            "ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[232]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=10):\n",
        "  \"\"\"\n",
        "  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n",
        "  \"\"\"\n",
        "  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
        "  transforms = model_weights.transforms()\n",
        "  model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "  # Adjust the number of output features in model to match the number of classes in the dataset\n",
        "  model.fc = torch.nn.Linear(in_features=2048,\n",
        "                             out_features=num_classes)\n",
        "  return model, transforms\n",
        "\n",
        "model, transforms = create_model()"
      ],
      "metadata": {
        "id": "U_-Q-mFWdu4A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Checking the memory of the GPU"
      ],
      "metadata": {
        "id": "NeWNuKKddw7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available GPU memory and total GPU memory\n",
        "total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZrPKF2agL_p",
        "outputId": "922fce6e-a95c-45bb-b7b0-d58de9361c68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total free GPU memory: 14.759 GB\n",
            "Total GPU memory: 15.835 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "if total_free_gpu_memory_gb >= 16:\n",
        "  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "  IMAGE_SIZE = 224\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n",
        "else:\n",
        "  BATCH_SIZE = 32\n",
        "  IMAGE_SIZE = 128\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKv-8KXpgNt6",
        "outputId": "d844b40f-15e2-4590-e9da-a72fdd376463"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory available is 14.759 GB, using batch size of 32 and image size 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.crop_size = IMAGE_SIZE\n",
        "transforms.resize_size = IMAGE_SIZE\n",
        "print(f\"Updated data transforms:\\n{transforms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDn-ImsIgP1d",
        "outputId": "74ef020a-8900-4dab-8a0b-c2e98c62cd6f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated data transforms:\n",
            "ImageClassification(\n",
            "    crop_size=128\n",
            "    resize_size=128\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDm3GZbBgSeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}